{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca421af5-e7d8-4f54-9b16-b2c805f261a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple GNN from scratch using PyTorch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# protein classification dataset from Hugging Face\n",
    "df = pd.read_json(\"hf://datasets/graphs-datasets/PROTEINS/full.jsonl\", lines=True)\n",
    "\n",
    "# exclude one row where num of nodes doesn't match\n",
    "df = df[df.edge_index.map(lambda a: len(np.unique(a[1]))) == df.node_feat.map(len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17e0a722-6c87-411c-b629-14ad73cea4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper\n",
    "def block_diag_pad(arrays):\n",
    "\n",
    "    total_size = sum(array.shape[0] for array in arrays)\n",
    "    result = np.zeros((total_size, total_size))\n",
    "    row_start = 0\n",
    "    col_start = 0\n",
    "    \n",
    "    for array in arrays:\n",
    "        size = array.shape[0]\n",
    "        result[row_start:row_start + size, col_start:col_start + size] = array\n",
    "        row_start += size\n",
    "        col_start += size\n",
    "    \n",
    "    return result\n",
    "\n",
    "def combine_list(ls):\n",
    "    return reduce(lambda a,b: a+b, ls)\n",
    "    \n",
    "def random_index_generator(max_len, batch_size):\n",
    "    shuffled_idxs = np.random.permutation(max_len)\n",
    "    for i in range(len(shuffled_idxs))[::batch_size]:\n",
    "        yield shuffled_idxs[i:i+batch_size]\n",
    "\n",
    "def batch_generator(adj_mat_list, feature_list, labels_vec, batch_size):\n",
    "    \n",
    "    for idxs in random_index_generator(len(labels_vec), batch_size):\n",
    "        \n",
    "        actual_batch_size = torch.tensor(len(idxs))\n",
    "\n",
    "        A_batch = block_diag_pad([adj_mat_list[i] for i in idxs])\n",
    "        X_batch = combine_list([feature_list[i] for i in idxs])\n",
    "        batch_index = [[i]*len(adj_mat_list[idx]) for i, idx in zip(range(batch_size), idxs)]\n",
    "\n",
    "        # cast into tensor\n",
    "        A_batch = torch.tensor(A_batch).float()\n",
    "        X_batch = torch.tensor(X_batch).float()\n",
    "        Y_batch = torch.tensor(labels_vec[idxs]).float()\n",
    "        batch_index = torch.tensor(combine_list(batch_index))\n",
    "        \n",
    "        yield A_batch, X_batch, Y_batch, batch_index, actual_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9db4cb01-c685-4a28-8b99-7b9ec23c98b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mat_list = []\n",
    "feature_list = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "\n",
    "    edge_pairs = list(zip(*df.edge_index.iloc[i]))\n",
    "    graph = nx.Graph(edge_pairs)\n",
    "\n",
    "    # normalized adjacency matrix\n",
    "    A = nx.adjacency_matrix(graph).toarray()\n",
    "    D = np.diag(np.sum(A, axis=1))\n",
    "    D_ = np.diag(1.0 / np.sqrt(np.diag(D)))\n",
    "    D_[np.isinf(D_)] = 0\n",
    "    A_ = D_ @ A @ D_\n",
    "\n",
    "    adj_mat_list.append(A_)\n",
    "    feature_list.append(df.node_feat.iloc[i])\n",
    "\n",
    "labels_vec = np.eye(2)[[i[0] for i in df.y]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08c10a10-3295-4c0e-99be-ed9fe227977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GNNLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        out = A @ X\n",
    "        out = self.linear(out)\n",
    "        return F.relu(out)\n",
    "\n",
    "class BasicGNN(nn.Module):\n",
    "\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        self.GNNlayer1 = GNNLayer(3, 32)\n",
    "        self.GNNlayer2 = GNNLayer(32, 64)\n",
    "        self.GNNlayer3 = GNNLayer(64, 64)\n",
    "        self.readoutLayer = nn.Linear(64,2)\n",
    "\n",
    "    def forward(self, A, X, batch_size, batch_index):\n",
    "        \n",
    "        # message propogation\n",
    "        H = self.GNNlayer1(A, X)\n",
    "        H = self.GNNlayer2(A, H)\n",
    "        H = self.GNNlayer3(A, H)\n",
    "\n",
    "        # graph level mean\n",
    "        out = torch.zeros((batch_size, H.size(1)))\n",
    "        for graph_id in range(batch_size):\n",
    "            mask = batch_index == graph_id\n",
    "            out[graph_id] = H[mask].mean(dim=0)\n",
    "\n",
    "        out = self.readoutLayer(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "model = BasicGNN()\n",
    "model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01eedfb9-47e2-42f4-a2c1-ecf54a593b89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80], Loss: 0.0809\n",
      "Epoch [2/80], Loss: 0.0805\n",
      "Epoch [3/80], Loss: 0.0800\n",
      "Epoch [4/80], Loss: 0.0794\n",
      "Epoch [5/80], Loss: 0.0788\n",
      "Epoch [6/80], Loss: 0.0783\n",
      "Epoch [7/80], Loss: 0.0777\n",
      "Epoch [8/80], Loss: 0.0769\n",
      "Epoch [9/80], Loss: 0.0763\n",
      "Epoch [10/80], Loss: 0.0756\n",
      "Epoch [11/80], Loss: 0.0753\n",
      "Epoch [12/80], Loss: 0.0748\n",
      "Epoch [13/80], Loss: 0.0742\n",
      "Epoch [14/80], Loss: 0.0740\n",
      "Epoch [15/80], Loss: 0.0738\n",
      "Epoch [16/80], Loss: 0.0735\n",
      "Epoch [17/80], Loss: 0.0730\n",
      "Epoch [18/80], Loss: 0.0727\n",
      "Epoch [19/80], Loss: 0.0728\n",
      "Epoch [20/80], Loss: 0.0726\n",
      "Epoch [21/80], Loss: 0.0724\n",
      "Epoch [22/80], Loss: 0.0724\n",
      "Epoch [23/80], Loss: 0.0720\n",
      "Epoch [24/80], Loss: 0.0718\n",
      "Epoch [25/80], Loss: 0.0718\n",
      "Epoch [26/80], Loss: 0.0716\n",
      "Epoch [27/80], Loss: 0.0715\n",
      "Epoch [28/80], Loss: 0.0712\n",
      "Epoch [29/80], Loss: 0.0713\n",
      "Epoch [30/80], Loss: 0.0710\n",
      "Epoch [31/80], Loss: 0.0709\n",
      "Epoch [32/80], Loss: 0.0709\n",
      "Epoch [33/80], Loss: 0.0712\n",
      "Epoch [34/80], Loss: 0.0712\n",
      "Epoch [35/80], Loss: 0.0710\n",
      "Epoch [36/80], Loss: 0.0710\n",
      "Epoch [37/80], Loss: 0.0706\n",
      "Epoch [38/80], Loss: 0.0709\n",
      "Epoch [39/80], Loss: 0.0708\n",
      "Epoch [40/80], Loss: 0.0706\n",
      "Epoch [41/80], Loss: 0.0708\n",
      "Epoch [42/80], Loss: 0.0708\n",
      "Epoch [43/80], Loss: 0.0707\n",
      "Epoch [44/80], Loss: 0.0707\n",
      "Epoch [45/80], Loss: 0.0707\n",
      "Epoch [46/80], Loss: 0.0707\n",
      "Epoch [47/80], Loss: 0.0705\n",
      "Epoch [48/80], Loss: 0.0705\n",
      "Epoch [49/80], Loss: 0.0705\n",
      "Epoch [50/80], Loss: 0.0707\n",
      "Epoch [51/80], Loss: 0.0706\n",
      "Epoch [52/80], Loss: 0.0704\n",
      "Epoch [53/80], Loss: 0.0707\n",
      "Epoch [54/80], Loss: 0.0705\n",
      "Epoch [55/80], Loss: 0.0705\n",
      "Epoch [56/80], Loss: 0.0705\n",
      "Epoch [57/80], Loss: 0.0705\n",
      "Epoch [58/80], Loss: 0.0704\n",
      "Epoch [59/80], Loss: 0.0706\n",
      "Epoch [60/80], Loss: 0.0702\n",
      "Epoch [61/80], Loss: 0.0700\n",
      "Epoch [62/80], Loss: 0.0701\n",
      "Epoch [63/80], Loss: 0.0701\n",
      "Epoch [64/80], Loss: 0.0702\n",
      "Epoch [65/80], Loss: 0.0702\n",
      "Epoch [66/80], Loss: 0.0701\n",
      "Epoch [67/80], Loss: 0.0703\n",
      "Epoch [68/80], Loss: 0.0705\n",
      "Epoch [69/80], Loss: 0.0700\n",
      "Epoch [70/80], Loss: 0.0704\n",
      "Epoch [71/80], Loss: 0.0701\n",
      "Epoch [72/80], Loss: 0.0700\n",
      "Epoch [73/80], Loss: 0.0701\n",
      "Epoch [74/80], Loss: 0.0703\n",
      "Epoch [75/80], Loss: 0.0702\n",
      "Epoch [76/80], Loss: 0.0698\n",
      "Epoch [77/80], Loss: 0.0699\n",
      "Epoch [78/80], Loss: 0.0699\n",
      "Epoch [79/80], Loss: 0.0699\n",
      "Epoch [80/80], Loss: 0.0700\n"
     ]
    }
   ],
   "source": [
    "# Loss function & optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "\n",
    "# training loop\n",
    "epochs = 80\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in batch_generator(adj_mat_list, feature_list, labels_vec, 8):\n",
    "        A_batch, X_batch, Y_batch, batch_index, batch_size = batch\n",
    "        \n",
    "        # call model\n",
    "        Y_hat = model(A_batch, X_batch, batch_size, batch_index)\n",
    "\n",
    "        # update\n",
    "        loss = criterion(Y_hat, Y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(df):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44fccb09-220a-41b2-96f9-2708e98b96b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.95      0.77       661\n",
      "           1       0.78      0.24      0.37       447\n",
      "\n",
      "    accuracy                           0.67      1108\n",
      "   macro avg       0.71      0.60      0.57      1108\n",
      "weighted avg       0.70      0.67      0.61      1108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for batch in batch_generator(adj_mat_list, feature_list, labels_vec, 16):\n",
    "        A_batch, X_batch, Y_batch, batch_index, batch_size = batch\n",
    "    \n",
    "        # call model\n",
    "        Y_hat = model(A_batch, X_batch, batch_size, batch_index)\n",
    "        y_true += list(torch.argmax(Y_batch, axis=1).numpy())\n",
    "        y_pred += list(torch.argmax(Y_hat, axis=1).numpy())\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cce06a-4776-4fc6-912f-5588b5f1c965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
